#The previous verison of this crashed in Google Chrome last night befoe I had a chance to save it, so I am going to do my best to 
#re-create it. 

#We are going to use different independent variables to determine if we can predict our dependent outcome variable - PoorCare 
##PoorCare = 1, GoodCare = 0

> setwd("~/R Studio Directory")
> setwd("~/R Studio Directory/LOGISTIC Regression")
> quality = read.csv("quality.csv")
> str(quality)
'data.frame':	131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...

##Let's inspect our PoorCare variable information:
> table(quality$PoorCare)

 0  1 
98 33 

##Before building any models, let's consider a simple baseline. 
##In a classification problem, A standard baseline method is to predict the most frequent outcome for all observations.
##So, we would predict that all patients are getting GoodCare. In this instance that would be 98 out of 131 patients, so we would be 
##right: 

> 98/131
[1] 0.7480916

##Approximtely 75% of the time. This is what we'll try to beat with our regression model

##In this example, we only have one dataset, so we want to randomly split our data set into a training data set and a testing data set
##so that we'll have a test set to measure our out-of-sample accuracy. We'll need to install a package to help us split these files:

> install.packages("caTools")
trying URL 'https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.3/caTools_1.17.1.tgz'
Content type 'unknown' length 182500 bytes (178 KB)
==================================================
downloaded 178 KB


The downloaded binary packages are in
	/var/folders/yh/s2b5w0pn1rg62k1hstvwswgm0000gn/T//RtmpaEA5qh/downloaded_packages

##Let's Load the Package:
> library(caTools)

##Now, let's randomly split the data into a training set and a data set. To make sure we get the same set as the tutorial, we must 
##set the seed which initializes the random number generator.

> set.seed(88)
> split = sample.split(quality$PoorCare, SplitRatio = 0.75)  
##75% of the data will go into the training set and 25% will go into the testing set 

> split
  [1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE
 [14]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
 [27]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
 [40]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE
 [53] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [66] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [79]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [92]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
[105]  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[118] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
[131] FALSE

##TRUE means and observation should go in the Training Set
##FALSE means an observation should go in the Testing Set

> qualityTrain = subset(quality, split == TRUE)
> qualityTest = subset(quality, split == FALSE)

> nrow(qualityTrain)
[1] 99

> nrow(qualityTest)
[1] 32

##Now we're ready to build a Logistic regression model using OfficeVistis and Narcotics as indepenent variables 

> QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
> summary(QualityLog)

Call:
glm(formula = PoorCare ~ OfficeVisits + Narcotics, family = binomial, 
    data = qualityTrain)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.06303  -0.63155  -0.50503  -0.09689   2.16686  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -2.64613    0.52357  -5.054 4.33e-07 ***
OfficeVisits  0.08212    0.03055   2.688  0.00718 **   ## <- We want to focus on the Coefficients Table here. This gives the estimate values
Narcotics     0.07630    0.03205   2.381  0.01728 *    ## <- (Betas) for the model. We can see by the stars that they're both signifigant
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 111.888  on 98  degrees of freedom
Residual deviance:  89.127  on 96  degrees of freedom
AIC: 95.127

Number of Fisher Scoring iterations: 4

##The last thing we want to look at in this model is the AIC Value - this is a measure of the quality of the model, like the Adjusted 
##R- Squared. It accounts for the number of variables used compared to the number of observations. It can only be compared between models
##on the same data set, but it provides a means for model selection. the preferred model is the one with the minimum AIC. 

##Let's make predictions on the dataset:

> predictTrain = predict(QualityLog, type="response")  ## <- type ="response" tells the predict function to give us probabilitites
> summary(predictTrain)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.06623 0.11910 0.15970 0.25250 0.26760 0.98460 

##Let's see if we're predicting higher probabilities for the actual PoorCare cases, like we'd expect:
> tapply(predictTrain, qualityTrain$PoorCare, mean)  ## <- this will compute the average prediction for each of the TRUE outcomes
        0         1 
0.1894512 0.4392246  	## <- this is good, it looks like we're predicting a higher probability for the actual PoorCare cases.

##Now that we have our model, let's assesss the accuracy of our predictions
##We can do this by using threshold values to compute a Confusion Matrix (aka Classification Matrix) which looks like this:

	         [Predicted = 0]       [Predicted = 1] 
	
[Actual = 0]|  True Negatives (TN) | False Positives (FP)
               ------------------------------------------
[Actual = 1]|  False Negatives(FN) | True Positives (TP)

##We can compute two outcome measures that help us determine what types of errors we're making:

##Sensitivity = True Positives / (True Positives + False Negatives). Also called the True Positive Rate. Measures the percentage of actual 
PoorCare cases that we classify correctly. 

##Specificity = True Negatives / (True Negatives + False Positives). Also called the True Negative Rate. Measures the percentage of actual
GoodCare cases we measure corectly. 

##A Model with a higher threshold will have a LOWER Sensitivity and a HIGHER Specificity
##A Model with a lower threshold will have a HIGHER Sensitivity and a LOWER Specificity

Let's compute some confusion matrices using differnt threshold values:

> table(qualityTrain$PoorCare, predictTrain > 0.5)

##For 70 cases, we predict GoodCare and they actually recieve GoodCare and for 10 cases we predict PoorCare and they actually recieve 
##PoorCare. We make 4 mistakes where we say PoorCare and it's actually GoodCare and we make 15 mistakes where we predict GoodCare but it's 
##actually PoorCare

    FALSE TRUE
  0    70    4
  1    15   10  ## <- Sensitivity = True Positives / (True Positives + False Negatives)
> 10/25
[1] 0.4 

##We have a Sensitivity of 0.4

##Our Specificity is: 
> 70/74      ## <- Specificity = True Negatives / (True Negatives + False Positives)
[1] 0.9459459


##Now, let's try to increase our threshold. 
> table(qualityTrain$PoorCare, predictTrain > 0.7)
   
    FALSE TRUE
  0    73    1
  1    17    8
  
> 8/25
[1] 0.32

> 73/74
[1] 0.9864865

##When we INCREASE our threshold, our Sensitivity goes down and our Specificity goes up 

##Now, let's decrease our threshold:
> table(qualityTrain$PoorCare, predictTrain > 0.2)
   
    FALSE TRUE
  0    54   20
  1     9   16
  
> 16/25
[1] 0.64

> 54/74
[1] 0.7297297

##When we DECREASE our threshold, our Sensitivity goes up and our Specificity goes down.

##Which threshold should we pick?
##A Reciever Operator Characteristic Curve -or- ROC Curvecan help us decide which value of the threshold is best. 
##The True Positive Rate (Sensitivity) is on the Y-Axis of the curve. Can be thought of as PoorCare Cases caught.
##The False Positive Rate (1-Specificity) is on the X-Axis. 

##The ROC Curve Always starts at the point (0,0) which is equal to a threshold value of 1. If threshold is 1, you will NOT catch any
##PoorCare cases (Sensitivity = 0). But, you will CORRECTLY label all of the GoodCare Cases. meaning you have a False Positive Rate of 0. 

##The ROC Curve Always ends at the point (1,1) which is equal to a threshold value of 0. If threshold is 0, you will catch ALL of the 
##PoorCare cases (Sensitivity = 1). But, you will INCORRECTLY label all of the GoodCare Cases as PoorCare cases. 
##Meaning you have a False Positive Rate of 1. 

##The threshold decreases as you move up the curve from (0,0) to (1,1). At the point (0,0.4) you're correctly labelling about 40% of the 
##PoorCare cases with a very small False Positive Rate. If you go along towards the other end of the curve at the point (0.6,0.9). You're
##correctly labelling about 90% of the PoorCare cases, but have a False Positive Rate of about 60%. Closer tothe middle at point 
##(0.3,0.8) you're correctly labelling about 80% of the PoorCare cases with a 30% False Positive Rate.

##The ROC Curve captures all Thresholds simultaneously

##Higher Threshold:
##High Specificity
##Low Sensitivity

##Lower Threshold:
##Low Specificity
##High Sensitivity

##You want to choose the best threshold fo the best tradeoff:
##Cost of failing to detect False Positives
##Cost of raising false alarms

##Here's how we generate ROC Curves:

> install.packages("ROCR")
also installing the dependency ‘gplots’

trying URL 'https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.3/gplots_3.0.1.tgz'
Content type 'unknown' length 511025 bytes (499 KB)
==================================================
downloaded 499 KB

trying URL 'https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.3/ROCR_1.0-7.tgz'
Content type 'unknown' length 150503 bytes (146 KB)
==================================================
downloaded 146 KB


The downloaded binary packages are in
	/var/folders/yh/s2b5w0pn1rg62k1hstvwswgm0000gn/T//RtmpaEA5qh/downloaded_packages
	
	
> library(ROCR)
Loading required package: gplots

Attaching package: ‘gplots’

The following object is masked from ‘package:stats’:

    lowess

> #we made predictions on our training set and called them predictTrain, let's use these predictions to make our ROC Curve
> ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)

> #Now, we ned to use the performance function, this defines what we'd like to plot on the X and Y axis on our ROC Curve
> ROCRperf = performance(ROCRpred, "tpr", "fpr")

##Now let's plot our curve
> plot(ROCRperf)

> #We can add colors by adding one additional argument to the plot function
> plot(ROCRperf, colorize=TRUE)

> #finally, let's add threshold labels to our plot
> plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
##Plot came out nicely

##Now let's go over how to assess the strength of our model. 
##We need to watch out for Multicolinearity

##How do we deal with that?
##Check the correlation of the independent variables. Itf they're excessively high, there may be mulitcolinerity. 
##Also checke whether or not they are Positive or Negative. If it agrees with intuition, we're okay. 

##We also need to look at the signifigance. 
##For that, we look at the Area Under the Curve -or- AUC for short. This shows an absolute measure of quality of prediction. 
##The AUC in our model is 77.5 %. We definitely Always want to be above 50% - which is the random chance you always have of being right
##or wrong. So ultimately as long as our AUC is over the baseline that we predicted at the beginning of the exercise, then it is a 
##suitable model we should use. 
